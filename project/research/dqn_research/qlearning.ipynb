{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7fb2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_states = 16\n",
    "n_actions = 4\n",
    "goal_state = 15\n",
    "\n",
    "Q_table = np.zeros((n_states,n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2a1403d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db50279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A learning rate is a variable by which the model decides how much it should chose to update its weights during the training phase. \n",
    "learning_rate = 0.8\n",
    "# Low discount factor means that I only care about immediate rewards whereas high discount factor means that I care about future rewards \n",
    "# A simple example would be if I pick the coin in front of me or I take a risky path that could lead to future rewards\n",
    "discount_factor = 0.95\n",
    "# An exploration probability is used to decide wether we will select a random action or the best know action. It is different from discount factor\n",
    "exploration_prob = 0.2\n",
    "# An epoch means that the model has passed through an entire phase of learning with all the available training data. \n",
    "# If there are 100 training data samples, then one epoch is when your model trains from all 100 once.\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a575829",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    current_state = np.random.randint(0,n_states)\n",
    "    while current_state != goal_state:\n",
    "        # Here the reason we have this set up in this way is because we try to balance exploration vs exploitation\n",
    "        # Here we arent manually defining wether to explore or exploit, rather we are deciding it based on a random value \n",
    "        # \"np.random.rand() < exploration_prob\"\n",
    "        # So if it is less than exploration proabability then explore else exploit.\n",
    "        \n",
    "        # there is a difference between exploration probability and discount factor. The exploration probability is used for you to decide wether you will pick a action based on wether you want to explore ( random action ) or exploit ( best known action ) but discount factor is used during the evaulation phase, it is used to evaluate wether your agent will be short sighted (care about immediate rewards) or long sighted (care about future rewards)\n",
    "        if np.random.rand() < exploration_prob:\n",
    "            action = np.random.randint(0, n_actions)\n",
    "        else:\n",
    "            action = np.argmax(Q_table[current_state])\n",
    "        # move to the next state, or wrap around -> used in circular queue\n",
    "        next_state = ( current_state + 1 ) % n_states\n",
    "        reward = 1 if next_state == goal_state else 0\n",
    "        \n",
    "        # So the way the following part of the formula “(reward + discount_factor * np.max(Q_table[next_state]) - Q_table[current_state, action])” works, is that it adds the rewards experienced ( for the current action and selected state ) with the rewards that could be earned in the future by the next state ( it choses the max action ) depending on how much you value immediate rewards or future rewards ( discount factor ) and we subtract that with the currently held assumption of the reward that occured by selecting the current state and the current action.\n",
    "        Q_table[current_state,action] += learning_rate * (reward + discount_factor * np.max(Q_table[next_state]) - Q_table[current_state, action])\n",
    "        \n",
    "        current_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05c31a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a problem in the above code which is that irrespective of the action I do ( after selecting the state ), I always end up in the next state depending on the itteration. \n",
    "\n",
    "# So my action does not determine the next state I always go to the next state in the itteration \"next_state = ( current_state + 1 ) % n_states\"\n",
    "\n",
    "# another thing is that I do not train my agent to learn about the last state ( 16th state) because according to my current logic the state selection goes sequentially ( 1 then 2 then 3 ) and it goes on until it reaches the goal which is 15 and then it restarts it so the last row never gets executed and hence the model knows nothing about that world. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2809052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.48767498 0.48377358 0.39013998 0.46816797]\n",
      " [0.51330923 0.51330923 0.51334208 0.51334203]\n",
      " [0.54036009 0.54036008 0.5403255  0.54036008]\n",
      " [0.56880009 0.5687998  0.56880009 0.56880009]\n",
      " [0.59873694 0.59873694 0.59873541 0.59873694]\n",
      " [0.63024941 0.63024941 0.63024941 0.63024941]\n",
      " [0.66342043 0.66342043 0.66342043 0.66342043]\n",
      " [0.6983373  0.6983373  0.6983373  0.6983373 ]\n",
      " [0.73509189 0.73509189 0.73509189 0.73509189]\n",
      " [0.77378094 0.77378094 0.77378094 0.77378094]\n",
      " [0.81450625 0.81450625 0.81450625 0.81450625]\n",
      " [0.857375   0.857375   0.857375   0.857375  ]\n",
      " [0.9025     0.9025     0.9025     0.9025    ]\n",
      " [0.95       0.95       0.95       0.95      ]\n",
      " [1.         1.         1.         1.        ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# so the Q table contains knowledge about which action in which state yeilds the maximum reward and it is used for the agent to create a model of the world \n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66484f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.35391026  9.25641026  8.35391026  9.25641026]\n",
      " [ 7.93621474  8.79358974  7.93621474  8.79358974]\n",
      " [ 7.53940367  8.35391026  7.53940012  8.35391026]\n",
      " [ 7.16197607  7.93621474  7.16241377  7.93588486]\n",
      " [ 6.80430978  7.5369914   6.79861174  7.53940401]\n",
      " [ 6.46409651  5.78717628  6.46409638  7.16243381]\n",
      " [ 6.80431212  6.13106626  6.80340703  6.67154943]\n",
      " [ 7.16243381  6.4640932   7.16243381  6.46409651]\n",
      " [ 7.53940401  6.80431212  7.53940401  6.80431212]\n",
      " [ 7.93621474  7.16242009  7.93621474  7.16243381]\n",
      " [ 8.35391026  7.53940401  8.35391026  7.53940401]\n",
      " [ 8.79358974  7.93621474  8.79358974  7.93621474]\n",
      " [ 9.25641026  8.35391026  9.25641026  8.35391026]\n",
      " [ 9.74358974  8.79358974  9.74358974  8.79358974]\n",
      " [10.25641026  9.25641026 10.25641026  9.25641026]\n",
      " [ 8.79358974  9.74358974  8.79358974  9.74358974]]\n"
     ]
    }
   ],
   "source": [
    "# this code solved the above issues\n",
    "\n",
    "n_states = 16\n",
    "n_actions = 4\n",
    "goal_state = 15\n",
    "\n",
    "Q_table = np.zeros((n_states,n_actions))\n",
    "\n",
    "\n",
    "learning_rate = 0.8\n",
    "discount_factor = 0.95\n",
    "exploration_prob = 0.2\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    current_state = np.random.randint(0,n_states)\n",
    "    count = 0\n",
    "    while count != n_states:\n",
    "        if np.random.rand() < exploration_prob:\n",
    "            action = np.random.randint(0, n_actions)\n",
    "        else:\n",
    "            action = np.argmax(Q_table[current_state])\n",
    "        \n",
    "        if action % 2 == 0: next_state = ( current_state + 1 ) % n_states\n",
    "        else: next_state = current_state - 1\n",
    "        \n",
    "        reward = 1 if next_state == goal_state else 0\n",
    "        \n",
    "        Q_table[current_state,action] += learning_rate * (reward + discount_factor * np.max(Q_table[next_state]) - Q_table[current_state, action])\n",
    "        \n",
    "        current_state = next_state\n",
    "        count += 1\n",
    "        \n",
    "print(Q_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
